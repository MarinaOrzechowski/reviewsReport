{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REVIEWS SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException, ElementClickInterceptedException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape TrustPilot \n",
    "\n",
    "~23 pages. Need to scrape a page, and move to the next one. Repeat until reach last page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringToDict(text):\n",
    "    parts = text.split('\",\"')\n",
    "    vocab = {}\n",
    "    \n",
    "    for i in range(len(parts)):\n",
    "        parts[i] = parts[i].replace('\\n{\"', '')\n",
    "        parts[i] = parts[i].replace('}\\n', '')\n",
    "        pair = parts[i].split('\":')\n",
    "        pair[0] = pair[0].replace('\"', '')\n",
    "        pair[1] = pair[1].replace('\"', '')\n",
    "        vocab[pair[0]] = pair[1]\n",
    "    return vocab\n",
    "\n",
    "df = pd.DataFrame(columns=('name', 'date', 'stars', 'text', 'boaDate', 'boaText', 'source'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [C:\\Users\\mskac\\.wdm\\drivers\\geckodriver\\win64\\v0.28.0\\geckodriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........Opened page 2\n",
      ".........Opened page 3\n",
      ".........Opened page 4\n",
      ".........Opened page 5\n",
      ".........Opened page 6\n",
      ".........Opened page 7\n",
      ".........Opened page 8\n",
      ".........Opened page 9\n",
      ".........Opened page 10\n",
      ".........Opened page 11\n",
      ".........Opened page 12\n",
      ".........Opened page 13\n",
      ".........Opened page 14\n",
      ".........Opened page 15\n",
      ".........Opened page 16\n",
      ".........Opened page 17\n",
      ".........Opened page 18\n",
      ".........Opened page 19\n",
      ".........Opened page 20\n",
      ".........Opened page 21\n",
      ".........Opened page 22\n",
      ".........Opened page 23\n",
      "No more pages left\n",
      "processed all pages\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n",
    "url = 'https://www.trustpilot.com/review/www.bankofamerica.com'\n",
    "\n",
    "temp = 2\n",
    "driver.implicitly_wait(5)\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "xpath1 = '/html/body/div[4]/div[3]/div/div/div[3]/button'\n",
    "xpath2 = '/html/body/div[3]/a'\n",
    "# close accept cookies tab\n",
    "driver.implicitly_wait(15)\n",
    "\n",
    "# close Covid notification tab\n",
    "try:\n",
    "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, xpath2))).click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, xpath1))).click()\n",
    "except TimeoutException:\n",
    "    pass\n",
    "\n",
    "driver.implicitly_wait(15)\n",
    "\n",
    "# close Covid notification tab\n",
    "try:\n",
    "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, xpath2))).click()\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "\n",
    "while True:   \n",
    "    try:\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "        for block in soup.find_all(\"article\", class_='review'):\n",
    "            # name, rating, review\n",
    "            script = block.find(\"script\").string\n",
    "            vocab = stringToDict(script)\n",
    "            name = vocab['consumerName']\n",
    "            stars = int(vocab['stars'])\n",
    "            text = vocab['reviewHeader']+vocab['reviewBody']\n",
    "\n",
    "            #date\n",
    "            dateBlock = block.find('div', class_='review-content-header__dates')\n",
    "            dateScript = dateBlock.find(\"script\").string\n",
    "            date = dateScript.split('\"')[3].split('T')[0]\n",
    "            y, m, d = date.split('-')\n",
    "            date = '/'.join([m, d, y])\n",
    "\n",
    "            df = df.append({'name' : name , 'date' : date, 'stars': stars, 'text': text, 'boaDate': None, 'boaText': None, 'source': 'trustpilot.com'} , ignore_index=True)        \n",
    "        \n",
    "        next_page_btn = driver.find_element_by_partial_link_text(\"Next page\")\n",
    "        # open next page\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.PARTIAL_LINK_TEXT, 'Next page'))).click()\n",
    "        except  StaleElementReferenceException:\n",
    "            driver.implicitly_wait(5)\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.PARTIAL_LINK_TEXT, 'Next page'))).click()\n",
    "        print('.........Opened page', temp)\n",
    "        temp += 1\n",
    "    except:\n",
    "        print(\"No more pages left\")\n",
    "        break\n",
    "print('processed all pages')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/trustpilot.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape BBB.com\n",
    "\n",
    "A single page, but needs 'load more' button to be pressed until scroll to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=('name', 'date', 'stars', 'text', 'boaDate', 'boaText', 'source'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Getting latest mozilla release info for v0.28.0\n",
      "[WDM] - Trying to download new driver from https://github.com/mozilla/geckodriver/releases/download/v0.28.0/geckodriver-v0.28.0-win64.zip\n",
      "[WDM] - Driver has been saved in cache [C:\\Users\\mskac\\.wdm\\drivers\\geckodriver\\win64\\v0.28.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      ".........Loaded more reviews\n",
      "Done loading reviews\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# press load more until reach end of the page\n",
    "driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n",
    "url = 'https://www.bbb.org/us/nc/charlotte/profile/bank/bank-of-america-0473-100421/customer-reviews'\n",
    "driver.implicitly_wait(5)\n",
    "driver.maximize_window()\n",
    "driver.get(url)\n",
    "\n",
    "xpath = '/html/body/div[2]/div[2]/div/button'\n",
    "css_selector = 'button.MuiButton-root:nth-child(2)'\n",
    "\n",
    "\n",
    "try:\n",
    "    # close cookies notification\n",
    "    myElem = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, xpath)))\n",
    "    myElem.click()\n",
    "except TimeoutException:\n",
    "    pass\n",
    "driver.implicitly_wait(15)\n",
    "\n",
    "while True:   \n",
    "    try:   \n",
    "        # load more reviews\n",
    "        myElem = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.CSS_SELECTOR, css_selector)))\n",
    "        myElem.click()\n",
    "        print('.........Loaded more reviews')\n",
    "        sleep(5) # otherwise spinning circle blocks the button\n",
    "    except TimeoutException:\n",
    "        print(\"Done loading reviews\")\n",
    "        break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reviews = []\n",
    "for block in soup.find_all(\"div\", class_='MuiGrid-root styles__Review-sc-1azxajg-0 fyMiFZ dtm-review MuiGrid-container'):\n",
    "    stars = block.find_all(d='M259.3 17.8L194 150.2 47.9 171.5c-26.2 3.8-36.7 36.1-17.7 54.6l105.7 103-25 145.5c-4.5 26.3 23.2 46 46.4 33.7L288 439.6l130.7 68.7c23.2 12.2 50.9-7.4 46.4-33.7l-25-145.5 105.7-103c19-18.5 8.5-50.8-17.7-54.6L382 150.2 316.7 17.8c-11.7-23.6-45.6-23.9-57.4 0z')\n",
    "    stars = len(stars)\n",
    "    date = block.find(\"p\", class_='MuiTypography-root Typography-y2r0fa-0 kpIiVF MuiTypography-body2').text\n",
    "    name = block.find('p', class_='MuiTypography-root Name-t42m9k-0 kSwwPu MuiTypography-body2').text\n",
    "    text = block.find_all('div', class_='MuiTypography-root Text-sc-12c66pm-0 fgbKlJ MuiTypography-body2')\n",
    "    \n",
    "    if len(text)>1:\n",
    "        boaDate = block.find('p', class_='MuiTypography-root Date-sc-8slhbi-0 kEubpt MuiTypography-body1').text\n",
    "        boaText = text[1].text\n",
    "    else:\n",
    "        boaDate = None\n",
    "        boaText = None\n",
    "        \n",
    "    review = text[0].text\n",
    "    \n",
    "    df = df.append({'name' : name , 'date' : date, 'stars': stars, 'text': review, 'boaDate': boaDate, 'boaText': boaText, 'source': 'bbb.org'} , ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>boaDate</th>\n",
       "      <th>boaText</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scott o</td>\n",
       "      <td>11/21/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>don't you set up an account with Bank of Ameri...</td>\n",
       "      <td>11/27/2020</td>\n",
       "      <td>Thank you for sharing your experience. At Bank...</td>\n",
       "      <td>bbb.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Samy  A</td>\n",
       "      <td>11/17/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>Worst bank ever. I been a customer with Bank o...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>bbb.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sharmell B</td>\n",
       "      <td>11/15/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>Bank of America in McDonough ga 30253\\n\\nHas s...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>bbb.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pam M</td>\n",
       "      <td>11/12/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>They have absolutely HORRIBE CUSTOMER SERVICE!...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>bbb.org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gulshan G</td>\n",
       "      <td>11/04/2020</td>\n",
       "      <td>1</td>\n",
       "      <td>Have a checking account with direct deposit fo...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>bbb.org</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name        date stars  \\\n",
       "0     scott o  11/21/2020     1   \n",
       "1     Samy  A  11/17/2020     1   \n",
       "2  Sharmell B  11/15/2020     1   \n",
       "3       Pam M  11/12/2020     1   \n",
       "4   Gulshan G  11/04/2020     1   \n",
       "\n",
       "                                                text     boaDate  \\\n",
       "0  don't you set up an account with Bank of Ameri...  11/27/2020   \n",
       "1  Worst bank ever. I been a customer with Bank o...        None   \n",
       "2  Bank of America in McDonough ga 30253\\n\\nHas s...        None   \n",
       "3  They have absolutely HORRIBE CUSTOMER SERVICE!...        None   \n",
       "4  Have a checking account with direct deposit fo...        None   \n",
       "\n",
       "                                             boaText   source  \n",
       "0  Thank you for sharing your experience. At Bank...  bbb.org  \n",
       "1                                               None  bbb.org  \n",
       "2                                               None  bbb.org  \n",
       "3                                               None  bbb.org  \n",
       "4                                               None  bbb.org  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'data\\bbb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape DepositAccounts.com\n",
    "\n",
    "1. Click 'View more'\n",
    "2. Find and click all 'Read More'\n",
    "3. Result: single page with reviews, so just scrape it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [C:\\Users\\mskac\\.wdm\\drivers\\geckodriver\\win64\\v0.28.0\\geckodriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n",
    "url = 'https://www.depositaccounts.com/banks/bank-of-america.html'\n",
    "\n",
    "driver.implicitly_wait(5)\n",
    "driver.maximize_window()\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "readMoreClassName = 'textExpand'\n",
    "\n",
    "# click 'View more'\n",
    "try:   \n",
    "    \n",
    "    viewMoreBtn = driver.find_element_by_partial_link_text(\"View MORE\")\n",
    "    viewMoreBtn.click()\n",
    "except TimeoutException:\n",
    "    print(\"Couldn't open all reviews page\")\n",
    "\n",
    "sleep(10)\n",
    "\n",
    "# expand all reviews\n",
    "more_buttons = driver.find_elements_by_class_name(readMoreClassName)\n",
    "for x in range(len(more_buttons)):\n",
    "    if more_buttons[x].is_displayed():\n",
    "        driver.execute_script(\"arguments[0].click();\", more_buttons[x])\n",
    "        time.sleep(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=('name', 'date', 'stars', 'text', 'boaDate', 'boaText', 'source'))\n",
    "\n",
    "page_source = driver.page_source\n",
    "soup = BeautifulSoup(page_source, 'lxml')\n",
    "starClassRe = re.compile('^stars')\n",
    "\n",
    "for block in soup.find_all(\"div\", class_='bankReviewContainer'):\n",
    "    title = block.find('h3').text\n",
    "    \n",
    "    stars = block.find('div', {\"class\" : starClassRe}).get('class')\n",
    "    stars = int(stars[1][-1])\n",
    "    \n",
    "    user = block.find('span', itemprop='author').text\n",
    "    date = block.find('span', itemprop='datePublished').get('datetime')\n",
    "    y, m, d = date.split('-')\n",
    "    date = '/'.join([m, d, y])\n",
    "    \n",
    "    text = block.find('p', itemprop='description').text\n",
    "    review = title+' '+text\n",
    "    \n",
    "    df = df.append({'name' : user , 'date' : date, 'stars': stars, \\\n",
    "                    'text': review, 'boaDate': None, 'boaText': None, 'source': 'depositaccounts.com'} , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'data\\depositaccounts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape ConsumerAffairs.com\n",
    "\n",
    "1. Click read full review btn\n",
    "2. Process data\n",
    "3. Click Next btn\n",
    "4. Repeat 1-3 until no Next btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path=GeckoDriverManager().install())\n",
    "url = 'https://www.consumeraffairs.com/finance/bofa.html'\n",
    "\n",
    "driver.implicitly_wait(5)\n",
    "driver.maximize_window()\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=('name', 'date', 'stars', 'text', 'boaDate', 'boaText', 'source'))\n",
    "\n",
    "monthVocab = {'jan': 1, 'feb': 2, 'march':3 , 'april':4, 'may': 5, 'june':6, 'july':7 , 'aug':8, 'sept':9 ,'oct':10, 'nov': 11,'dec':12}\n",
    "page = 2\n",
    "\n",
    "while True:   \n",
    "    try:\n",
    "        #expend all reviews\n",
    "        more_buttons = driver.find_elements_by_partial_link_text('Read full review')\n",
    "        \n",
    "        for x in range(len(more_buttons)):\n",
    "            if more_buttons[x].is_displayed():\n",
    "                driver.execute_script(\"arguments[0].click();\", more_buttons[x])\n",
    "                time.sleep(1)\n",
    "        # process data on current page        \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "        for block in soup.find_all(\"div\", class_='rvw js-rvw'):\n",
    "            \n",
    "            stars = block.find('meta', itemprop = 'ratingValue').get('content')\n",
    "            stars = int(stars)\n",
    "            \n",
    "            user = block.find('strong', itemprop = 'author').text\n",
    "            user = user.split('of')[0]\n",
    "            \n",
    "            date = block.find('span', class_ = 'ca-txt-cpt').text.split(': ')[1]\n",
    "            date = date.replace('.', '').replace(',', '').lower()\n",
    "            m, d, y = date.split(' ')\n",
    "            m = str(monthVocab[m])\n",
    "            date = '/'.join([m.zfill(2), d.zfill(2), y])\n",
    "            \n",
    "            text = block.find('div', class_ = 'rvw-bd').find_all('p')[1].text\n",
    "\n",
    "            df = df.append({'name' : user.strip() , 'date' : date, 'stars': stars, 'text': text, 'boaDate': None, 'boaText': None, 'source': 'consumeraffairs.com'} , ignore_index=True)        \n",
    "        \n",
    "        next_page_btn = driver.find_element_by_partial_link_text(\"Next\")\n",
    "        # open next page\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.PARTIAL_LINK_TEXT, 'Next'))).click()\n",
    "        except  StaleElementReferenceException:\n",
    "            sleep(2)\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.PARTIAL_LINK_TEXT, 'Next'))).click()\n",
    "        print('.........Opened next page', page)\n",
    "        page += 1\n",
    "    except:\n",
    "        print(\"No more pages left\")\n",
    "        break\n",
    "print('processed all pages')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'data\\consumeraffairs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Wallethub.com\n",
    "\n",
    "Using Scraping Robot saved locally all pages.\n",
    "Scraping from local htmls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "mypath = 'data\\htmls'\n",
    "files = [mypath+'\\\\'+ f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "print(len(files))\n",
    "df = pd.DataFrame(columns=('name', 'date', 'stars', 'text', 'boaDate', 'boaText', 'product', 'source'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    soup = BeautifulSoup(open(file,  encoding=\"utf8\").read())\n",
    "\n",
    "    for block in soup.find_all('article', itemprop = 'review'):\n",
    "        user = block.find('span', itemprop='author').text\n",
    "        date = block.find('time', class_='rvtab-ci-time regular-font').get('datetime')\n",
    "        stars = block.find('meta', itemprop='ratingValue').get('content')\n",
    "        text = block.find('div', itemprop='description').text\n",
    "        product = block.find('div', class_='rvtab-ci-category')\n",
    "        if product:\n",
    "            product = product.text\n",
    "            product = re.split('<.span>', product)\n",
    "            product = product[0].split(': ')[1]\n",
    "        else:\n",
    "            product = None\n",
    "\n",
    "        df = df.append({'name' : user.strip() , 'date' : date, 'stars': int(stars), 'text': text, \\\n",
    "                        'boaDate': None, 'boaText': None, 'product': product, 'source': 'wallethub.com'}\\\n",
    "                       , ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2440"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data\\wallethub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
